{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-737ecffa804edefe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# LIN 373 Machine Learning Toolbox for Text Analysis\n",
    "\n",
    "# Homework 2 - due Monday Sep 26 2022 at 11:59pm\n",
    "\n",
    "For this homework you will hand in (upload) to canvas your code named ``hw2_YourEID.ipynb``.\n",
    "\n",
    "__Before submitting__, please reset your kernel and rerun everything from the beginning (`Kernel` >> `Restart and Run All`) an ensure your code outputs the correct answer.\n",
    "\n",
    "A perfect solution for this homework is worth **100** points, and there is a bonus problem at the end that is worth **5 points**. For non-programming tasks, you must show work to get partial credit. For programming tasks, make sure that your code can run using Python 3.5+. If you cannot complete a problem, include as much pseudocode as possible for partial credit. However, make sure it does not have any output errors. **If there are any output errors, half of the points for that problem will be automatically deducted.**\n",
    "\n",
    "Collaboration: you are free to discuss the homework assignments with other students and work towards solutions together.  However, all of the code you write must be your own! There is a channel on Slack where you can look for a study group.\n",
    "\n",
    "Review extension and academic dishonesty policy here: https://jessyli.com/courses/lin373n\n",
    "\n",
    "For typing up your answers to problems 1, 2 and 3, information can be found about Markdown cells for Jupyter Notebooks here: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html\n",
    "\n",
    "\n",
    "### Please list any collaborators here:\n",
    "- Ethan Glass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e27014b61dce338",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem 1: Naive Bayes\n",
    "\n",
    "We'll do a bit of manual parameter estimation here.\n",
    "\n",
    "## **(a)** (8 points) \n",
    "Calculate the sufficient parameters for Naive Bayes using the data in the figure below, that\n",
    "is, the prior class probabilities and the conditional probabilities for all of the symbols.\n",
    "\n",
    "doc | X | Y\n",
    "-- | --| --\n",
    "d1 | a b b b c b | +\n",
    "d2 | c a c c c b | -\n",
    "d3 | c c c b | -\n",
    "d4 | b a b b b | +\n",
    "d5 | b c a b b | ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "* P(+) = 1/2\n",
    "* P(-) = 1/2\n",
    "* P(a | +) = 2/11\n",
    "* P(a | -) = 1/10\n",
    "* P(b | +) = 8/11\n",
    "* P(b | -) = 2/10\n",
    "* P(c | +) = 1/11\n",
    "* P(c | -) = 7/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(b)** (7 points) \n",
    "Using these, calculate the most likely class (pos or neg) for the unlabeled example (d5, labeled ???)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that d5 is positive: 0.0031791171740628748\n",
      "Probability that d5 is negative: 0.00028\n",
      "d5 is more likely to be positive.\n"
     ]
    }
   ],
   "source": [
    "pos_prob = (1/2 * 2/11 * 1/11 * 8/11 * 8/11 * 8/11)\n",
    "neg_prob = (1/2 * 1/10 * 2/10 * 2/10 * 2/10 * 7/10)\n",
    "print('Probability that d5 is positive: {}'.format(pos_prob))\n",
    "print('Probability that d5 is negative: {}'.format(neg_prob))\n",
    "print(\"d5 is more likely to be positive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Language Modeling\n",
    "\n",
    "In this problem, we will implement our own Shakespeare and Austen language models!\n",
    "\n",
    "### Data\n",
    "\n",
    "We will be working with a few different files under the `data` subdirectory. \n",
    "- `shakespeare.txt`\n",
    "- `shakespeare_short.txt`\n",
    "- `austen.txt`\n",
    "- `austen_short.txt`\n",
    "- `test_short.txt`\n",
    "\n",
    "<mark>You should use the \"short\" text files to do testing and debugging</mark> as the other files will take a little while (but should be under 10 mins for a 8G RAM laptop) to run! \n",
    "\n",
    "### Note re NLTK\n",
    "\n",
    "The goal of this problem is to think about how to implement language models from first principles. Thus, **the *ONLY* allowable functions from NTLK in this problem are `sent_tokenize` and `word_tokenize`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Clean text. (5 points) \n",
    "\n",
    "Create a function `sent_transform(sent_string)`, such that when given a sentence as a string, it would return a `list` of words, lower-cased. Use NLTK's `word_tokenize` function to tokenize the sentence. For now, you can use any random string to test this function.\n",
    "\n",
    "```\n",
    ">>> sent_transform(\"ROSALIND. Why, whither shall we go?\")\n",
    "['rosalind', '.', 'why', ',', 'whither', 'shall', 'we', 'go', '?']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rosalind', '.', 'why', ',', 'whither', 'shall', 'we', 'go', '?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def sent_transform(sent_string):\n",
    "    \n",
    "    sent_string = sent_string.lower()\n",
    "    \n",
    "    return word_tokenize(sent_string)\n",
    "\n",
    "sent_transform(\"ROSALIND. Why, whither shall we go?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Find n-grams. (10 points) \n",
    "\n",
    "Create a function `make_ngram_tuples(words, n)` that returns a sequence of all n-grams seen in the input, in order.  The function returns a sequence of tuples where each tuple is of the form `(context, word)`.  The context is a tuple of the preceding n−1 words for each word; if the number of preceding words is smaller than n−1, place a `<s>` symbol in place of each missing context.  Close each sentence with an additional token `</s>`.  You can assumen n>1, that is, we are interested in enumerating bigrams, trigrams etc, and not unigrams.\n",
    "\n",
    "For now, you can use any random string to test this function.\n",
    "\n",
    "```\n",
    ">>> samples = [’she’, ’eats’, ’happily’ ’.’]\n",
    ">>> make_ngram_tuples(samples, 2)\n",
    "[((’<s>’,), ’she’), ((’she’,), ’eats’), ((’eats’,), ’happily’), ((’happily’,), ’.’), ((’.’,), ’</s>’)]\n",
    ">>> make_ngram_tuples(samples, 3)\n",
    "\n",
    "[((’<s>’, ’<s>’), ’she’), ((’<s>’, ’she’), ’eats’), ((’she’, ’eats’), ’happily’),((’eats’, ’happily’), ’.’), ((’happily’, ’.’), ’</s>’)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('<s>',), 'she'), (('she',), 'eats'), (('eats',), 'happily'), (('happily',), '.'), (('.',), '</s>')]\n",
      "\n",
      "[(('<s>', '<s>'), 'she'), (('<s>', 'she'), 'eats'), (('she', 'eats'), 'happily'), (('eats', 'happily'), '.'), (('happily', '.'), '</s>')]\n"
     ]
    }
   ],
   "source": [
    "def make_ngram_tuples(words, n):\n",
    "    \n",
    "    tuples = []\n",
    "    length = len(words)\n",
    "    starter = ()\n",
    "    counter = 1\n",
    "\n",
    "    # adds n - 1 sentence starters and appends the first tuple to tuples\n",
    "    for num in range(n - 1): \n",
    "        starter += ('<s>',)\n",
    "    starter = (starter, words[0])\n",
    "    tuples.append(starter)\n",
    "        \n",
    "    # iterates through each tuple in the list\n",
    "    for item in tuples:\n",
    "        first_item = () # this will be the first item in new_tuple\n",
    "    \n",
    "        # if n > 2, the loop will get all elements (except the first) from the first element, which is a tuple, in item\n",
    "        for num in range(1, n - 1):\n",
    "            first_item += (item[0][num],)\n",
    "\n",
    "        # add the second element in item to first_item\n",
    "        first_item += (item[1],)\n",
    "        \n",
    "        if counter < length:\n",
    "            # new_tuple will be a nested tuple\n",
    "            # first element is first_item, a tuple\n",
    "            # second element is the next word \n",
    "            new_tuple = (first_item, words[counter])\n",
    "            tuples.append(new_tuple)\n",
    "            counter += 1\n",
    "        \n",
    "        # we have reached the last word and need to end it with </s>\n",
    "        else: \n",
    "            new_tuple = (first_item, '</s>')\n",
    "            tuples.append(new_tuple)\n",
    "            break\n",
    "            \n",
    "    return tuples\n",
    "\n",
    "print(make_ngram_tuples(['she', 'eats', 'happily', '.'], 2))\n",
    "print()\n",
    "print(make_ngram_tuples(['she', 'eats', 'happily', '.'], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)  Building an n-gram language model\n",
    "\n",
    "We are now ready to estimate a bigram language model from the training data.  We will use **Laplace smoothing (add-1)**.\n",
    "\n",
    "### (c1) Process the training file (8 points)\n",
    "\n",
    "We will first need to transform a file of plain text into a list of tokenized \"sentences\", where each \"sentence\" is a list of *lower-cased* words. Implement a function `process_text(textfile)` to do so. Assume that the path of `textfile` is of a form like `data/austen_short.txt`, i.e., relative paths. Make sure that you call `sent_transform` in this function.\n",
    "\n",
    "```\n",
    ">>> processed_sents = process_text(\"data/austen_short.txt\")\n",
    ">>> print(processed_sents[10])\n",
    "['he', 'considered', 'the', 'blessing', 'of', 'beauty', 'as', 'inferior', 'only', 'to', 'the', 'blessing', 'of', 'a', 'baronetcy', ';', 'and', 'the', 'sir', 'walter', 'elliot', ',', 'who', 'united', 'these', 'gifts', ',', 'was', 'the', 'constant', 'object', 'of', 'his', 'warmest', 'respect', 'and', 'devotion', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'considered', 'the', 'blessing', 'of', 'beauty', 'as', 'inferior', 'only', 'to', 'the', 'blessing', 'of', 'a', 'baronetcy', ';', 'and', 'the', 'sir', 'walter', 'elliot', ',', 'who', 'united', 'these', 'gifts', ',', 'was', 'the', 'constant', 'object', 'of', 'his', 'warmest', 'respect', 'and', 'devotion', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def process_text(textfile):\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    with open(textfile, encoding='utf-8' ) as f:\n",
    "        content = f.read() \n",
    "        tokenized_sentences = sent_tokenize(content)\n",
    "        for sentence in tokenized_sentences:\n",
    "            transformed_sentences = sent_transform(sentence)\n",
    "            sentences.append(transformed_sentences)\n",
    "  \n",
    "    return sentences\n",
    "\n",
    "processed_sents = process_text(r\"C:\\Users\\veron\\OneDrive\\Documents\\LIN 373N\\HW2\\data\\data\\austen_short.txt\")\n",
    "print(processed_sents[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c2) Vocabulary (8 points).\n",
    "\n",
    "The language model ought to be able to handle words not seen in the training data.  Such words will most certainly appear if the LM is used in some application to estimate the likelihood of new data.   There  are  many  ways  to  incorporate  unknown  vocabulary  in  a  language  model.   In  this problem, we will take all words that appear only once and replace them with the symbol `<UNK>`. Thus, at test time, if we encounter an out-of-vocabulary word, we can resort to replacing the word with `<UNK>` which has well-formed probabilities.\n",
    "\n",
    "First, implement a function `get_vocab(tokenized_sents)` where the parameter `tokenized_sents` is a list of tokenized \"sentences\" (where each \"sentence\" is a list of lower-cased words) returned by the function `process_text`. The function `get_vocab` will return a `set` of all unique words in `tokenized_sents` that appeared more than once.\n",
    "Note: do not forget to add three extra tokens to the vocabulary: `<s>`, `</s>`, and `<UNK>`.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    ">>> vocab = get_vocab(processed_sents)\n",
    ">>> print(len(vocab))\n",
    "302\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def get_vocab(tokenized_sents):\n",
    "    \n",
    "    multiple_words = set() # create an empty set\n",
    "    all_words = [word for sentence in tokenized_sents for word in sentence] # combine all sentences into one list\n",
    "    \n",
    "    counts = Counter(all_words) # use counter to get counts of words\n",
    "    multiples = [key for key, value in counts.items() if value > 1] # make a list 'multiples' of words w/ more than 1 count\n",
    "    \n",
    "    multiple_words.update(multiples) # add multiples\n",
    "    multiple_words.update(['<s>', '</s>', '<UNK>']) # add the three extra tokens\n",
    "    \n",
    "    return multiple_words\n",
    "\n",
    "vocab = get_vocab(processed_sents)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c3) Process unknown words (8 points).\n",
    "\n",
    "Write a function `process_unk(tokenized_sents, vocab)` that takes in (1) tokenized sentences returned by `process_text`, and (2) a vocabulary (set of words) returned by `get_vocab`. The function returns a list of tokenized sentences that is the same as `tokenized_sents` except that all words appearing only once will be replaced by the symbol `<UNK>`.\n",
    "\n",
    "```\n",
    ">>> print(processed_sents[3])\n",
    "['of', 'south', 'park', ',', 'in', 'the', 'county', 'of', 'gloucester', ',', 'by', 'which', 'lady', '(', 'who', 'died', '1800', ')', 'he', 'has', 'issue', 'elizabeth', ',', 'born', 'june', '1', ',', '1785', ';', 'anne', ',', 'born', 'august', '9', ',', '1787', ';', 'a', 'still-born', 'son', ',', 'november', '5', ',', '1789', ';', 'mary', ',', 'born', 'november', '20', ',', '1791', '.', \"''\"]\n",
    ">>> processed_unk = process_unk(processed_sents, vocab)\n",
    ">>> print(processed_unk[3])\n",
    "['of', '<UNK>', '<UNK>', ',', 'in', 'the', 'county', 'of', '<UNK>', ',', 'by', 'which', 'lady', '(', 'who', 'died', '<UNK>', ')', 'he', 'has', '<UNK>', 'elizabeth', ',', 'born', '<UNK>', '1', ',', '<UNK>', ';', 'anne', ',', 'born', '<UNK>', '<UNK>', ',', '<UNK>', ';', 'a', '<UNK>', 'son', ',', 'november', '<UNK>', ',', '<UNK>', ';', 'mary', ',', 'born', 'november', '<UNK>', ',', '<UNK>', '.', \"''\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of', 'south', 'park', ',', 'in', 'the', 'county', 'of', 'gloucester', ',', 'by', 'which', 'lady', '(', 'who', 'died', '1800', ')', 'he', 'has', 'issue', 'elizabeth', ',', 'born', 'june', '1', ',', '1785', ';', 'anne', ',', 'born', 'august', '9', ',', '1787', ';', 'a', 'still-born', 'son', ',', 'november', '5', ',', '1789', ';', 'mary', ',', 'born', 'november', '20', ',', '1791', '.', \"''\"]\n",
      "['of', '<UNK>', '<UNK>', ',', 'in', 'the', 'county', 'of', '<UNK>', ',', 'by', 'which', 'lady', '(', 'who', 'died', '<UNK>', ')', 'he', 'has', '<UNK>', 'elizabeth', ',', 'born', '<UNK>', '1', ',', '<UNK>', ';', 'anne', ',', 'born', '<UNK>', '<UNK>', ',', '<UNK>', ';', 'a', '<UNK>', 'son', ',', 'november', '<UNK>', ',', '<UNK>', ';', 'mary', ',', 'born', 'november', '<UNK>', ',', '<UNK>', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "def process_unk(tokenized_sents, vocab):\n",
    "    \n",
    "    new_tokenized_sents = []\n",
    "    \n",
    "    for sentence in tokenized_sents:\n",
    "        \n",
    "        new_sentence = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            \n",
    "            if word in vocab:\n",
    "                new_sentence.append(word)\n",
    "            else:\n",
    "                new_sentence.append('<UNK>')\n",
    "        \n",
    "        new_tokenized_sents.append(new_sentence)\n",
    "    \n",
    "    return new_tokenized_sents\n",
    "\n",
    "print(processed_sents[3])\n",
    "processed_unk = process_unk(processed_sents, vocab)\n",
    "print(processed_unk[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c4) N-gram frequencies (10 points).\n",
    "\n",
    "We now need to get the frequency counts -- which will allow us to calculate the conditional frequency distribution for our language model. Write a function `get_freq_dist(tokenized_sents, n)` that takes in (1) a list of tokenized sentences (such as one returned by `process_unk`), and (2) the number `n` that denotes the order of the n-gram (`n=2` means bigram, `n=3` means trigram, etc). The function will return a dictionary `freqdict` (`freqdict` can be a `Counter`) such that `freqdict[context][word]` records the number of times `word` follows `context`. You can think of `context` as the first element of a tuple in a list returned by `make_ngram_tuples`, and `word` as the second element of that tuple.\n",
    "\n",
    "```\n",
    ">>> freqdict = get_freq_dict(processed_unk, 2)\n",
    ">>> print(freqdict[('of',)])\n",
    "Counter({'<UNK>': 25, 'the': 17, 'his': 6, 'her': 6, 'kellynch': 4, 'a': 4, 'being': 4, 'their': 3, 'it': 3, 'charles': 2, 'somerset': 2, 'any': 2, 'very': 2, 'mind': 2, 'life': 2, 'all': 2, 'mr': 2, 'them': 2, 'himself': 1, 'mary': 1, 'high': 1, 'baronet': 1, 'sir': 1, 'character': 1, 'real': 1, 'ever': 1, 'respectability': 1, '.': 1, 'youth': 1, 'and': 1, 'elliot': 1, 'inferior': 1, 'again': 1, 'economy': 1, 'which': 1, ';': 1})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'<UNK>': 25, 'the': 17, 'his': 6, 'her': 6, 'kellynch': 4, 'a': 4, 'being': 4, 'their': 3, 'it': 3, 'charles': 2, 'somerset': 2, 'any': 2, 'very': 2, 'mind': 2, 'life': 2, 'all': 2, 'mr': 2, 'them': 2, 'himself': 1, 'mary': 1, 'high': 1, 'baronet': 1, 'sir': 1, 'character': 1, 'real': 1, 'ever': 1, 'respectability': 1, '.': 1, 'youth': 1, 'and': 1, 'elliot': 1, 'inferior': 1, 'again': 1, 'economy': 1, 'which': 1, ';': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_freq_dict(tokenized_sents, n):\n",
    "    \n",
    "    freqdict = defaultdict(Counter)\n",
    "    all_ngram = []\n",
    "    \n",
    "    # make ngrams of each sentence, then combine them all into one list \n",
    "    for sentence in tokenized_sents:\n",
    "        all_ngram += make_ngram_tuples(sentence, n)\n",
    "    \n",
    "    for context, word in all_ngram:\n",
    "        freqdict[context][word] += 1\n",
    "        \n",
    "    return freqdict\n",
    " \n",
    "freqdict = get_freq_dict(processed_unk, 2)\n",
    "print(freqdict[('of',)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c5) The langauge model\n",
    "\n",
    "We are now ready to put everything together and make our langauge model! Below we have written the function `build_ngram_model(textfile, n)` that takes in a text file, the value `n` that signals the order of our n-gram, and returns a language model in the form of a `namedtuple`. All we need to do is to call the various functions you've implemented so far in (c)! (**No implementation required here**).\n",
    "\n",
    "A `namedtuple` is a versatile data structure that allows one to associate multiple data fields with one variable. Below, we created a `namedtuple` called `LanguageModel`; this `LanguageModel` consists of the following information: the n-gram order `n`, the frequency distribution dictionary `fd`, and the vocabulary (as a `set` of words) `vocab`. Now, after we make a `LanguageModel`, we will be able to access these fields using the `dot` operator, for example, `toy_lm.vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT MODIFY THIS CELL, but you need to run it.\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "def build_ngram_model(textfile, n):\n",
    "    LanguageModel = namedtuple('LanguageModel', ['n', 'fd', 'vocab'])\n",
    "    psents = process_text(textfile)\n",
    "    vocab = get_vocab(psents)\n",
    "    psentsunk = process_unk(psents, vocab)\n",
    "    fd = get_freq_dict(psentsunk, n)\n",
    "    return LanguageModel(n, fd, vocab)\n",
    "\n",
    "toy_lm = build_ngram_model(r\"C:\\Users\\veron\\OneDrive\\Documents\\LIN 373N\\HW2\\data\\data\\austen_short.txt\", 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c6) Log conditional probabilities. (10 points)\n",
    "\n",
    "The heart of the language model above is just the frequency dictionary. To make our model functional, we will need to use the frequency dictionary to calculate (log) conditional probabilities. Write a function `log_prob(lm, context, word)` that takes in a language model `lm`, `context` in the form of a tuple, and a `word`. The function returns the *add-1 (Laplacian) smoothed* conditional probability values `log P(word|context)`.\n",
    "\n",
    "We would like to calculate log probabilities, instead of raw probability values, because ultipmately we will use the language model to calculate the likelihood of a text. Multiplying many very small raw probability values may result in underflow issues (and inaccuracies) in any programming language.\n",
    "\n",
    "You will need to get the size of the vocabulary when writing this function. Keep in mind that the `<UNK>`, `<s>`, and `</s>` symbols should all be a part of your vocabulary.\n",
    "\n",
    "*Please use log with base 2 for this problem!*\n",
    "\n",
    "\n",
    "```\n",
    ">>> log_prob(toy_lm, ('.',), '</s>')\n",
    "-2.451695969857692\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.451695969857692\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def log_prob(lm, context, word):\n",
    "    \n",
    "    n = lm[0]\n",
    "    fd = lm[1]\n",
    "    vocab = lm[2]\n",
    "    size_vocab = len(lm[2])\n",
    "    \n",
    "    # count of word following context, + 1\n",
    "    numerator = fd[context][word] + 1 \n",
    "    # count of all context occurrences \n",
    "    denom = sum(fd[context].values()) + size_vocab\n",
    "    \n",
    "    log_prob = math.log(numerator / denom, 2)\n",
    "    \n",
    "    return log_prob\n",
    "    \n",
    "\n",
    "print(log_prob(toy_lm, ('.',), '</s>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c7) Perplexity (16 points)\n",
    "\n",
    "Our final step to make our langauge model functional would be to calculate perplexity of a document (e.g., a new text file). Implement the function `get_ppl(lm, newfile)` that returns the perplexity of `newfile` given language model `lm` that you built using `build_ngram_model`.\n",
    "\n",
    "Be mindful to check whether a word appears in the language model's vocabulary; if not, replace with `<UNK>`.\n",
    "\n",
    "```\n",
    ">>> get_ppl(toy_lm, \"data/test_short.txt\")\n",
    ">>> 51.98841239479734\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.988412394797635"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_ppl(lm, testfile):\n",
    "    \n",
    "    n = lm[0]\n",
    "    fd = lm[1]\n",
    "    vocab = lm[2]\n",
    "    \n",
    "    test_size = 0\n",
    "    test_ngram = []\n",
    "    sum_log_prob = 0\n",
    "    \n",
    "    # process text \n",
    "    test = process_text(testfile)\n",
    "    \n",
    "    # replace words in testfile with <UNK> \n",
    "    process_test = process_unk(test, vocab)\n",
    "            \n",
    "    # add sentence starter and ender to each length of sentence\n",
    "    # also create ngrams for each sentence\n",
    "    for sentence in test:\n",
    "        test_size += len(sentence) + 2 # length of sentence plus sentence start and end\n",
    "    \n",
    "    # create freqdict \n",
    "    test_freq = get_freq_dict(process_test, n)\n",
    "    \n",
    "    for context_key in test_freq:\n",
    "        for word_key in test_freq[context_key]:\n",
    "            count = test_freq[context_key][word_key]\n",
    "            prob = log_prob(lm, context_key, word_key) \n",
    "            sum_log_prob += (prob * count)\n",
    "    \n",
    "    return 2 ** ((sum_log_prob*(-1)) / test_size)\n",
    "    \n",
    "    \n",
    "\n",
    "get_ppl(toy_lm, r\"C:\\Users\\veron\\OneDrive\\Documents\\LIN 373N\\HW2\\data\\data\\test_short.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Authorship attribution (10 points)\n",
    "\n",
    "If we have built language models of multiple authors, they can be used to check the author of an unknown piece of writing. Concretely, for the unknown text, we can run the file through each known author's language model, and use perplexity to predict which author the unknown text is most likely to belong to.\n",
    "\n",
    "In this problem, build two **bigram** models:\n",
    "- a  Shakespeare language model\n",
    "- a  Jane Austen language model\n",
    "\n",
    "Make sure to train these models from the full text. Once you have trained both models, use the `get_ppl` function from each language model on the file `test.txt`. \n",
    "\n",
    "Who wrote the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511.31765795186305\n",
      "385.1354241879401\n"
     ]
    }
   ],
   "source": [
    "shakespeare = build_ngram_model(r\"C:\\Users\\veron\\OneDrive\\Documents\\LIN 373N\\HW2\\data\\data\\shakespeare.txt\", 2)\n",
    "austen = build_ngram_model(r\"C:\\Users\\veron\\OneDrive\\Documents\\LIN 373N\\HW2\\data\\data\\austen.txt\", 2)\n",
    "test = r\"C:\\Users\\veron\\OneDrive\\Documents\\LIN 373N\\HW2\\data\\data\\test.txt\"\n",
    "\n",
    "ppl_shakespeare = get_ppl(shakespeare, test)\n",
    "ppl_austen = get_ppl(austen, test)\n",
    "print(ppl_shakespeare)\n",
    "print(ppl_austen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lower perplexity indicates a better model, so it is more likely that Jane Austen wrote the test text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## (e) Bonus: Random text generator (5 points)\n",
    "\n",
    "Now, we are ready to generate some Shakespeare text! Starting with the sentence start symbol `<s>`, at each step, use the previously generated word as context, and sample one of the top 5 words that follow this word. We stop whenever we hit `</s>`, or when we have generated a 20-word sentence, whichever is earlier.\n",
    "\n",
    "Implement a function `text_generator(bigramlm, k)` that takes a bigram langauge model---trained on our Shakespeare text---as input, and generates `k` random sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "9ee92ef7ac9c4e76a3443c477335478f09402823207bba7eb8b055205da5967f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
