{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIN 373N Machine Learning Toolbox for Text Analysis\n",
    "\n",
    "# Homework 1 - due Friday Sep 9 2022 at 11:59pm\n",
    "\n",
    "For this homework you will hand in (upload) to canvas:\n",
    "- a notebook renamed ``hw1_YourEID.ipynb``\n",
    "\n",
    "__Before submitting__, please reset your kernel and rerun everything from the beginning (`Kernel` >> `Restart and Run All`) an ensure your code outputs the correct answer. \n",
    "\n",
    "A perfect solution for this homework is worth 95 points but will be counted out of 100. If you completed homework 0, you will automatically receive an additional 5 points.  For programming tasks, make sure that your code can run using Python 3.5+. If you cannot complete a problem, include as much pseudocode as possible for partial credit. However, make sure it does not have any output errors. **If there are any output errors, half of the points for that problem will be automatically deducted.**\n",
    "\n",
    "Collaboration: you are free to discuss the homework assignments with other students and work towards solutions together.  However, all of the code you write must be your own! There is a channel on Slack where you can look for a study group.\n",
    "\n",
    "Review extension and academic dishonesty policy here: https://jessyli.com/courses/lin373n\n",
    "\n",
    "For typing up your answers to problems 1, 2 and 3, information can be found about Markdown cells for Jupyter Notebooks here: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html\n",
    "\n",
    "\n",
    "### Please list any collaborators here:\n",
    "- Ethan Glass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: the paradox of induction (15 points)\n",
    "\n",
    "Consider a statement whose truth is unknown. If we see many examples that are compatible with it, we are tempted to view the statement as more probable. Such reasoning is often referred to as _inductive inference_ (in a philosophical, rather than mathematical sense). Consider now the statement that \"all cows are white\". An equivalent statement is that \"everything that is not white is not a cow\". We then observe several black panthers. Our observations are clearly compatible with the statement, but do they make the hypothesis \"all cows are white\" more likely?\n",
    "\n",
    "To analyze such a situation, we consider a probabilistic model. Let us assume that there are two possible states of the world, which we model as complementary events:\n",
    "\n",
    "<center>$A$: all cows are white,\n",
    "    \n",
    "<center>$A^c$: 50% of all cows are white.\n",
    "\n",
    "Let $p$ be the prior probability $P(A)$ that all cows are white. We make an observation of a cow or a panther, with probability $q$ and $1-q$, respectively, independent of whether event $A$ occurs or not. Assume that $0<p<1, 0<q<1$, and that all panthers are black.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Given the event $B=$\\{a black panther was observed\\}, what is $P(A|B)$? Show your work (5pts)\n",
    "\n",
    "### We know that $P(A|B) = \\frac{P(A \\cap  B)}{P(B)}$. We also know that $P(B)=(1-q)$ and that $P(A \\cap B) = p * (1-q)$. Thus, $P(A|B) = p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Given the event $C=$\\{a white cow was observed\\}, what was $P(A|C)$? Show your work (5pts)\n",
    "\n",
    "### We know that the formula for $P(A | C) = \\frac{P(A \\cap C)}{P(C)} = \\frac{P(A)P(C|A)}{P(C)}$. We know $P(A) = p$. We also know $P(C|A)$ since given the event A, the probability of observing a white cow is $q$, which is given. Now, we calculate $P(C)$ using the Law of total probability, since there are two events A and complement of A, as follows <br>\n",
    "$P(C) = P(A)*P(C|A) + P(A^c)*P(C|A^c)$ <br>\n",
    "$P(C) = pq + (1-p)(\\frac{q}{2})$ <br>\n",
    "### Since $A^c$ is the complement of A, then $P(A^c) = 1 - p$. We also know that given the event $A^c$, the probability of observing a white cow is $P(C|A^c) = q * \\frac{1}{2}$. Now, we can calculate $P(A|C)$ as follows <br>\n",
    "$P(A|C) = \\frac{p*q}{pq + (1-p)(\\frac{q}{2})}$ <br>\n",
    "$P(A|C) = \\frac{p*q}{pq + (\\frac{q-pq}{2})}$ <br>\n",
    "$P(A|C) = \\frac{p*q}{\\frac{pq+q}{2})}$ <br>\n",
    "$P(A|C) = \\frac{2*p*q}{pq+q}$ <br>\n",
    "$P(A|C) = \\frac{2*p*q}{q(p+1)}$ <br>\n",
    "$P(A|C) = \\frac{2*p}{p+1}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Which is larger? Explain the implication. (5pts)\n",
    "\n",
    "### Since we are multiplying 2 and p and adding 1 to p in the denominator, $P(A|C)$ will always be larger. This means that given that we observe a white cow, the probability that all cows are white is *larger* than if we had observed a black panther."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Problem 2: MAP (11 pts)\n",
    "\n",
    "We have discussed the Bernoulli distribution. Now, if you flip the coin $N$ times, with $H$ heads and $T$ tails, the likelihood of observing a sequence (data) $D$ is:\n",
    "$$\n",
    "P(D|\\theta) = \\theta^H(1-\\theta)^T\n",
    "$$\n",
    "\n",
    "In the Bayesian framework, we assume that we have some prior knowledge of $\\theta$ which can be described with a distribution $P(\\theta)$. \n",
    "The Beta distribution $Beta(\\alpha;\\beta)$ is often used as this prior, which, combined with our observed data $D$, leads to the posterior distribution $P(\\theta|D)$. \n",
    "\n",
    "But what if we want a single number (an estimate) of our \"best\" $\\theta$? This value will no longer be the same as the maximum likelihood estimate $\\hat{\\theta}_{MLE}$. Instead, this new \"best\" parameter, dubbed $\\hat{\\theta}_{MAP}$, is called _maximum a posterior_ or MAP: $$\\hat{\\theta}_{MAP}=\\text{argmax}_\\theta(P(\\theta|D)$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) What is $\\hat{\\theta}_{MAP}$? Express it in terms of $H,T,\\alpha,\\beta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we need $P(\\theta|D)$ to calculate $\\hat{\\theta}_{MAP}$. We know that <br>\n",
    "$P(\\theta|D) = \\frac{P(\\theta) * P(D | \\theta)}{P(D)}$ <br> \n",
    "### We ignore $P(D)$ since we are focused on maximizing $\\theta$, we use the Beta distribution (ignoring the Gamma constant) for $P(\\theta)$. <br>\n",
    "$\\hat(\\theta)_{MAP} = \\frac{H+\\alpha-1}{T+H+\\alpha+\\beta-2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3: Decision Trees (30 points)\n",
    "\n",
    "Consider the following set of training examples where we have two features, $X_1$ and $X_2$, and the goal is to predict the target $Y$. Each row indicates the values observed, and how many times that set of values was observed. For example, $(+,T,T)$ was observed 3 times, while $(−,T,T)$ was never observed.\n",
    "\n",
    "|Y | $$X_1$$ | $$X_2$$ | Count|\n",
    "|-|-|-|-|\n",
    "|+ | T | T | 3|\n",
    "|+ | T | F | 4|\n",
    "|+ | F | T | 4|\n",
    "|+ | F | F | 1|\n",
    "|- | T | T | 0|\n",
    "|- | T | F | 1|\n",
    "|- | F | T | 3|\n",
    "|- | F | F | 5 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) What is the sample entropy $H(Y)$ for this training data (with logarithms base 2), _before_ we start splitting on either feature? (10 pts)\n",
    "\n",
    "$H(Y) = -((\\frac{12}{21})log_2(\\frac{12}{21})-(1-(\\frac{12}{21}))log_2(1-(\\frac{12}{21}))$ <br>\n",
    "$H(Y) = 0.9852281360342516$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)  Should we first split on $X_1$ or $X_2$? Calculate the information gains for each feature. (20 pts)\n",
    "\n",
    "$H(Y | X_1|) = \\frac{8}{21}*-(\\frac{7}{8})log_2(\\frac{7}{8})-(1-\\frac{7}{8})log_2(1-\\frac{7}{8}) + \\frac{13}{21}*-(\\frac{5}{13})log_2(\\frac{5}{13})-(1-\\frac{5}{13})log_2(1-\\frac{5}{13})$ <br>\n",
    "$H(Y | X_2|) = \\frac{10}{21}*-(\\frac{7}{10})log_2(\\frac{7}{10})-(1-\\frac{7}{10})log_2(1-\\frac{7}{10}) + \\frac{11}{21}*-(\\frac{6}{11})log_2(\\frac{6}{11})-(1-\\frac{6}{11})log_2(1-\\frac{6}{11})$ <br>\n",
    "### Now, we calculate the gain by subtracting the conditional entropy from H(Y)<br>\n",
    "### gain(X_1) = 0.18310473570119645\n",
    "### gain(X_2) = 0.04488331134123025\n",
    "### So since the gain for x_1 is larger, we should split on X_1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 4:  log odds ratios (44 points)\n",
    "This exercise is an exploratory analysis of the Sentiment140 dataset. Sentiment140 combines 160K tweets collected via the Twitter API with most of the emoticons removed. Each tweet is annotated with polarity: positive (4), negative (0) and neutral (2). You do not have to check the original paper that proposed this dataset, but if you are curious, here is the link: [https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf](https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf).\n",
    "\n",
    "In this problem, we will analyze how often a word tend to appear with a positive sentiment vs. a negative one. The metric we are going to use is  **log odds ratio**, that compares the conditional probability of a word occurring in one type of sentences, say, positive ($P(word|pos)$), and the word occurring in another type of sentences, say, negative ($P(word|neg)$):\n",
    "$$log\\_odds\\_ratio(word, pos) = \\log\\frac{P(word|pos)}{P(word|neg)}$$\n",
    "The higher the $log\\_odds\\_ratio$, the more likely the word is associated with positive sentences.\n",
    "\n",
    "We will use the Sentiment140 dataset. Sentiment140 combines 1.6 million tweets collected via the Twitter API with most of the emoticons removed. Each tweet is annotated with polarity: positive (4), negative (0) or neutral (2). _We will  **not** consider neutral tweets in this problem_. You do not have to check the original paper that proposed this dataset, but if you are curious, here is the link: [https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf](https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf).\n",
    "\n",
    "Download from Canvas the file ``sentiment140_sample1.csv`` ---a 10K sample from the training set of Sentiment140---and put it under the  **same directory** (folder) as your python script or notebook file. As a reminder, the file is formatted under six fields, including polarity, tweet ID, date, query username and the text of the tweet. We will only use polarity and tweet text in this assignment.\n",
    "\n",
    "In the following exercises, we have provided several expected inputs and outputs of the functions that you will implement. Treat these as test cases for your code; if you get numbers very far off from what is listed here with the same input, you have bugs to crush."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1752285545</td>\n",
       "      <td>Sat May 09 21:30:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Doomed_Vampire</td>\n",
       "      <td>Doesn't know what to eat! What would you eat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1468206560</td>\n",
       "      <td>Tue Apr 07 00:17:43 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>rynequin</td>\n",
       "      <td>Doesn't know why, but is feeling very down. An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2179666845</td>\n",
       "      <td>Mon Jun 15 09:25:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Deeluvly</td>\n",
       "      <td>@OfficialTG3 you know I'm leaving Texas, right?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2219598618</td>\n",
       "      <td>Thu Jun 18 00:54:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>katyshepherdx</td>\n",
       "      <td>I really hate her.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1822816157</td>\n",
       "      <td>Sat May 16 20:27:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jennyconfetti</td>\n",
       "      <td>No one's replying to my texts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3               4  \\\n",
       "0  0  1752285545  Sat May 09 21:30:51 PDT 2009  NO_QUERY  Doomed_Vampire   \n",
       "1  0  1468206560  Tue Apr 07 00:17:43 PDT 2009  NO_QUERY        rynequin   \n",
       "2  0  2179666845  Mon Jun 15 09:25:05 PDT 2009  NO_QUERY        Deeluvly   \n",
       "3  0  2219598618  Thu Jun 18 00:54:58 PDT 2009  NO_QUERY   katyshepherdx   \n",
       "4  0  1822816157  Sat May 16 20:27:14 PDT 2009  NO_QUERY   jennyconfetti   \n",
       "\n",
       "                                                   5  \n",
       "0   Doesn't know what to eat! What would you eat ...  \n",
       "1  Doesn't know why, but is feeling very down. An...  \n",
       "2   @OfficialTG3 you know I'm leaving Texas, right?   \n",
       "3                               I really hate her.    \n",
       "4                     No one's replying to my texts   "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data\n",
    "import pandas\n",
    "sentiment_data = pandas.read_csv(\"sentiment140_sample1.csv\", header = None, encoding = \"ISO-8859-1\")\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.984400</td>\n",
       "      <td>2.001648e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.000039</td>\n",
       "      <td>1.921348e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.467817e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.957661e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.002835e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.177586e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.329052e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1\n",
       "count  10000.000000  1.000000e+04\n",
       "mean       1.984400  2.001648e+09\n",
       "std        2.000039  1.921348e+08\n",
       "min        0.000000  1.467817e+09\n",
       "25%        0.000000  1.957661e+09\n",
       "50%        0.000000  2.002835e+09\n",
       "75%        4.000000  2.177586e+09\n",
       "max        4.000000  2.329052e+09"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Frequency counts  (11 points)\n",
    "First, let's create dictionaries that record the count of each word in positive tweets, as well as the count of each word in negative tweets. Here, here, ``counts[\"pos\"]`` will contain key-value pairs of a word and its number of appearance in positive tweets, ``counts[\"neg\"]`` will contain key-value pairs of a word and its number of appearance in negative tweets\n",
    "\n",
    "To parse the tweets, we will use NLTK's ``word_tokenize()`` function. As an example, the following tokenizes a sentence into a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\veron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'sentence', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') #you only have to do this once per environment\n",
    "\n",
    "from nltk import word_tokenize\n",
    "word_tokenize(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower-casing all words gives cleaner counts. For example, consider the two sentences: \"Apples are delicious. John loves apples.\" If we do not lower-case each word, ''Apples'' and ''apples'' will be counted as two different words. In Python, you can lower-case a word by calling ``lower()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Apples becomes apples\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Apples\" == \"apples\")\n",
    "print(\"Apples becomes\", \"Apples\".lower())\n",
    "print(\"Apples\".lower() == \"apples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only consider words and not symbols or numbers. To test whether a word is a word, that is, consisting of only English characters, we can use ``isalpha()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"Apples\".isalpha())\n",
    "print(\"Apples123\".isalpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "31\n",
      "17\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "def get_counts(data):\n",
    "    \"\"\" \n",
    "    counts the number of times a word appears in negative or positive tweets\n",
    "    \n",
    "    Parameters:\n",
    "    data: Pandas dataframe of tweets\n",
    "    \n",
    "    Returns:\n",
    "    counts: Dictionary of counts, which includes the dictionaries 'pos' and 'neg'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    counts = {\"pos\":{}, \"neg\":{}}\n",
    "    posTweets = data.loc[data[0] == 4][5]\n",
    "    negTweets = data.loc[data[0] == 0][5]\n",
    "    \n",
    "    posWords = []\n",
    "    for tweet in posTweets:\n",
    "        lstWords = word_tokenize(tweet)\n",
    "        for word in lstWords:\n",
    "            if word.isalpha():\n",
    "                posWords.append(word.lower())\n",
    "    pos = Counter(posWords)\n",
    "    counts['pos'] = dict(pos)\n",
    "    \n",
    "    negWords = []\n",
    "    for tweet in negTweets:\n",
    "        lstWords = word_tokenize(tweet)\n",
    "        for word in lstWords:\n",
    "            if word.isalpha():\n",
    "                negWords.append(word.lower())\n",
    "    neg = Counter(negWords)\n",
    "    counts['neg'] = dict(neg)\n",
    "\n",
    "    return counts\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Do not change\n",
    "counts = get_counts(sentiment_data)\n",
    "print(counts[\"pos\"][\"happy\"]) # should print 115\n",
    "print(counts[\"neg\"][\"happy\"]) # should print 31\n",
    "print(counts[\"pos\"][\"hate\"]) # shuld print 17\n",
    "print(counts[\"neg\"][\"hate\"]) # should print 106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Calculating $P(\\text{word}|\\text{polarity})$ (11 points)\n",
    "\n",
    "Create a function ``get_word_prob(counts, word, polarity)``, where ``counts`` is a dictionary like in the previous task, ``word`` is the word for which $P(word|polarity)$ will be calculated, and ``polarity`` is either ``pos`` or ``neg``. The function should return $P(word|polarity)$. If ``counts[polarity]`` does not contain ``word``, then return 0.\n",
    "\n",
    "Note that you should NOT need to use the variable ``data`` here, and only rely on the three arguments of the function: ``counts, word, polarity``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0023902194802887643\n",
      "0.00025489167103980807\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def get_word_prob(counts, word, polarity):\n",
    "    \"\"\" \n",
    "    calculates the probability of a word given a polarity \n",
    "    \n",
    "    Parameters:\n",
    "    counts (dict): the dictionaries 'pos' and 'neg' which count word occurances\n",
    "    word (str): the word you want to get the probability for\n",
    "    polarity (str): wither 'pos' or 'neg'\n",
    "    \n",
    "    Returns:\n",
    "    probability (float):  the probability of a word given a polarity \n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        total = sum(counts[polarity].values()) \n",
    "        occur =  counts[polarity][word] # number of occurences of word\n",
    "        return occur / total\n",
    "    except:\n",
    "        return 0 \n",
    "\n",
    "\n",
    "\n",
    "#Do not change\n",
    "print(get_word_prob(counts, \"great\", \"pos\")) # should be ~0.00239\n",
    "print(get_word_prob(counts, \"glad\", \"neg\")) # should be ~0.000255\n",
    "print(get_word_prob(counts, \"wugs\", \"neg\")) # should be 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Calculate the log odds ratio of a word  (11 points)\n",
    "\n",
    "\n",
    "Using the above function, we can calculate $P(word|pos)$ and $P(word|neg)$ given a word, so we are ready to calculate the log odds ratio of that word as well. Create a function ``log_odds_ratio(count_dict, word, polarity)``, where the arguments are of the same type/format as in the previous problem. The function should return $log\\_odds\\_ratio(word)$:\n",
    "\n",
    "$$ log\\_odds\\_ratio(word, polarity) = \\log\\frac{P(word|polarity)}{P(word|opposite\\_polarity)} $$\n",
    "\n",
    "If the denominator is zero, return a very large number (please return 10000). Again you should NOT need to use the variable ``data`` here, and only rely on the three arguments of the function: ``counts``, ``word``, and ``polarity``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2873255481586012\n",
      "-0.07793487356431161\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def log_odds_ratio(counts, word, polarity):\n",
    "    \"\"\" \n",
    "    This function returns the log odds ratio of a term (see previous cell)\n",
    "    \n",
    "    Parameters:\n",
    "    counts (dict): the dictionaries 'pos' and 'neg' which count word occurances\n",
    "    word (str): the word you want to get the probability for\n",
    "    polarity (str): wither 'pos' or 'neg'\n",
    "    \n",
    "    Returns:\n",
    "    log_odds_ratio (float): log( prob(word|plarity) / P(word|opposity_polarity) )\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        numerator = get_word_prob(counts, word, polarity)\n",
    "        \n",
    "        if polarity == 'pos':\n",
    "            opposite = 'neg'\n",
    "        else:\n",
    "            opposite = 'pos'\n",
    "    \n",
    "        denominator = get_word_prob(counts, word, opposite)\n",
    "        \n",
    "        log_odds_ratio = math.log(numerator / denominator)\n",
    "    \n",
    "        return log_odds_ratio\n",
    "\n",
    "    except: \n",
    "        return 10000\n",
    "\n",
    "\n",
    "# Do not change\n",
    "print(log_odds_ratio(counts, \"great\", \"pos\")) # should be ~1.287\n",
    "print(log_odds_ratio(counts, \"the\", \"neg\")) #  should be ~-0.0779\n",
    "print(log_odds_ratio(counts, \"wug\", \"neg\")) # should be a very large number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Sorting log odds ratios (11 points)\n",
    "\n",
    "After being able to calculate log odds ratios for individual words, we can now sort words according to its association with a polarity class, say, positive. Create a function ``sort_pos_words(data)``, that takes in the entire dataframe as an argument, and return a sorted list of ``(word, log odds ratio)`` tuples for the positive sentiment class.\n",
    "\n",
    "If you implement this without filtering out any words, you will notice that there are many cases where the conditional probability of the denominator is 0, leading to the very large number you specified in the ``log_odds_ratio()`` function. This is because most words appear only once in the dataset. One way to mitigate this issue is to consider only words that appeared at least $x$ times in the dataset; here, let's only include words that appeared more than 10 times in the dataset, regardless of the polarity of the tweet (positive or negative).\n",
    "\n",
    "Use your function to print out the top 10 most positive (Note: we only consider those words appear in the positive sentiment class, namely you only need to sort the words in positive sentiment class and take the top-10 and bottom-10):\n",
    "\n",
    "`` [('followfriday', 10000), ('ff', 10000), ('proud', 10000), ('yummy', 3.1187445986382114), ('congrats', 3.0699544344687792), ('moon', 2.713279490530047), ('exciting', 2.639171518376325), ('gorgeous', 2.5591288107027883), ('welcome', 2.5165691962839927), ('prom', 2.4721174337131586)]``\n",
    "       \n",
    " and the top 10  most negative:\n",
    " \n",
    "`` [('bummed', -2.410684488873212), ('scared', -2.410684488873212), ('stomach', -2.4907271965467483), ('ugh', -2.5648351687004705), ('happened', -2.8702168182516523), ('worse', -2.9215101126392025), ('throat', -2.9703002768086346), ('died', -3.144653663953412), ('sad', -3.5203466137279067), ('hurts', -3.589339485214858)]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most positive \n",
      " [('followfriday', 10000), ('ff', 10000), ('proud', 10000), ('yummy', 3.1188253460361706), ('congrats', 3.0700351818667384), ('moon', 2.713360237928006), ('exciting', 2.639252265774284), ('gorgeous', 2.5592095581007475), ('welcome', 2.516649943681952), ('prom', 2.4721981811111178)]\n",
      "Top 10 most negative \n",
      " [('feeling', -1.2340299113374316), ('gone', -1.2474529316695722), ('damn', -1.256931675624116), ('bad', -1.2987462260571228), ('missed', -1.3528134473273985), ('wo', -1.3990028297967732), ('miss', -1.5154299330420207), ('wish', -1.630445183925678), ('hate', -1.7559228417431036), ('sick', -1.9704530753791987)]\n"
     ]
    }
   ],
   "source": [
    "def sort_pos_words(data):\n",
    "    \"\"\"\n",
    "    takes in a pandas dataframe and outputs the top 10 most positive and negative words in the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): the tweets in a dataframe\n",
    "    \n",
    "    Return:\n",
    "    sorted_list (list): a sorted list of (word, log odds ratio) tuples for the positive sentiment class\n",
    "    \n",
    "    \"\"\"\n",
    "    counts = get_counts(data)\n",
    "    posDict = {key:val for key, val in counts['pos'].items() if val > 10}\n",
    "    negDict = {key:val for key, val in counts['neg'].items() if val > 10}\n",
    "    \n",
    "#     for key in negDict:\n",
    "#         if key in posDict:\n",
    "#             negDict[key] += posDict[key]\n",
    "#         else: \n",
    "#             pass\n",
    "#     newDict = posDict | negDict\n",
    "    logDict = {}\n",
    "    \n",
    "    for key in posDict:\n",
    "        word = key\n",
    "        count = posDict[key]\n",
    "        num = log_odds_ratio(counts, word, \"pos\")\n",
    "        logDict[word] = num\n",
    "        \n",
    "    sortedLog = sorted(logDict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sortedLog\n",
    "    \n",
    "# Do not change\n",
    "lst = sort_pos_words(sentiment_data)\n",
    "print(\"Top 10 most positive \\n\", lst[:10]) # see previous cell for what this should print\n",
    "print(\"Top 10 most negative \\n\", lst[-10:])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
