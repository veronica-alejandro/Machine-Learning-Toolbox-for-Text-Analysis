{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIN 373N Machine Learning Toolbox for Text Analysis\n",
    "\n",
    "# Homework 3 - due Monday Oct 10 2022 at 11:59pm\n",
    "\n",
    "For this homework you will hand in (upload) to Canvas a notebook renamed ``hw3_YourEID.ipynb``.\n",
    "\n",
    "__Before submitting__, please reset your kernel and rerun everything from the beginning (`Kernel` >> `Restart and Run All`) an ensure your code outputs the correct answer.\n",
    "\n",
    "A perfect solution for this homework is worth **100** points. For programming tasks, make sure that your code can run using Python 3.5+. If you cannot complete a problem, include as much pseudocode as possible for partial credit. However, make sure it does not have any output errors.Â **If there are any output errors, half of the points for that problem will be automatically deducted.**\n",
    "\n",
    "Collaboration: you are free to discuss the homework assignments with other students and work towards solutions together.  However, all of the code you write must be your own! There is a channel on Slack where you can look for a study group.\n",
    "\n",
    "Review extension and academic dishonesty policy here: https://jessyli.com/courses/lin373n\n",
    "\n",
    "For typing up your answers to non-programming problems, information can be found about Markdown cells for Jupyter Notebooks here: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Logistic Regression\n",
    "\n",
    "* **(a)** (5 points) In logistic regression, we know that when there are two classes an example can belong to ($y\\in\\{0,1\\}$), for each example ${\\bf x}$ with M features,\n",
    "    $$\n",
    "    p(y=1|x_1, x_2, ..., x_M) = \\frac{1}{1+\\exp{ (-\\sum_{j=1}^M w_jx_j-b) }}\n",
    "    $$\n",
    "    $$\n",
    "    p(y=0|x_1, x_2, ..., x_M) = 1-p(y=1|x_1, x_2, ..., x_M)\n",
    "    $$\n",
    "How many parameters does logistic regression estimate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters we are estimating are the weights (w) and the bias term (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(b)** (10 points) Briefly describe, given a dataset of $N$ examples, how are these parameters estimated. You do not need to lay out specific mathematical derivations, rather, provide a summary of a few sentences of the process. You can assume that all these examples will be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal weights, we use the gradient descent algorithm. From a random x, we find the slope at that point and move either to the right if the slope is negative or to the left otherwise. We use a learning rate to decide how much we should move until eventually we find the minimum of the loss function. This will be done for the all $N$ examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(c)** (10 points) Logistic regression can be extended to classify $K$ classes instead of only 2. When $y$ can be one of K classes, the conditional probability of $y$ being of a particular class $k$ is:\n",
    " $$\n",
    " p(y=k|x_1,x_2,...,x_M) = \\frac{\\exp(\\sum_{j=1}^M w_{jk}x_j+b_k)}{\\sum_{k'=1}^K\\exp(\\sum_{j=1}^M w_{jk'}x_j+b_{k'})}\n",
    " $$\n",
    "How many parameters are we estimating in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate the same parameters, *w* and *b* but we estimate each *b* for every $k \\in K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Regularization and evaluation\n",
    "\n",
    "Recall that with Ridge Regression, we add a _regularization_ term to the log likelihood:\n",
    "$$\n",
    "    l({\\bf w}) = \\log \\prod_{i=1}^N{ p(y^{(i)}|{\\bf x^{(i)},w})}-\\lambda||{\\bf w}||_2^2\n",
    "$$\n",
    "\n",
    "* **(a)** (10 points) What is the goal of this regularization term? How does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this regularization term is to avoid overfitting by subtracting a penalty term which is squaring the weights and multiplying it by the lambda term. This penalizes large weights so that they don't have as much of an impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(b)** (15 points) Your friend David uses cross validation to pick $\\lambda$ for his application of logistic regression as follows. He sets aside 20\\% of his data as a test set, and then runs cross validation on his training data to get cross validation accuracy with multiple values of $\\lambda$. He also trains a model on the entire training data for each value of $\\lambda$ and computes test accuracy on his held out data. He gets the following results:\n",
    "\n",
    "| $\\lambda$ | Cross-validation,accuracy | Test accuracy |\n",
    "|:---------:|:-------------------------:|:-------------:|\n",
    "|     0     |            0.7            |      0.65     |\n",
    "|     1     |            0.75           |      0.7      |\n",
    "|     10    |            0.8            |      0.63     |\n",
    "|    100    |            0.7            |      0.6      |\n",
    "\n",
    "* **(b-i)** What is the optimal $\\lambda$ in terms of test accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal $\\lambda$ in terms of test accuracy is $\\lambda = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(b-ii)** Is this the same $\\lambda$ that would be chosen by cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of cross validation, the optimal $\\lambda$ is $\\lambda = 10$. So, no it would not be the same $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(b-iii)** David wants to inform the public about his model and his results. What should he report as the test accuracy of his method (logistic regression with regularization parameter $\\lambda$) on this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "David will report a test accuracy of 0.63, as it lines up with the optimal lambda chosen in terms of the cross validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Logistic Regression in sklearn\n",
    "\n",
    "Logistic regression, it turns out, is a natural \"cousin\" to Naive Bayes; specifically it is the \"discriminative\" variant. First, let's review the code for the sklearn version of [Naive Bayes (sentiment analysis notebook)](https://utexas.instructure.com/courses/1296690/files/folder/06-naivebayes). Note that we define a random state here, so that the answers will be consistent across everyone (it should be 0.8175)! \n",
    "\n",
    "Below is the code from our Naive Bayes (sentiment analysis) notebook which serves as a starting point for this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_model accuracy is :  0.8175\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82       197\n",
      "           1       0.82      0.82      0.82       203\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## PLEASE RUN THIS CELL BUT DO NOT MODIFY.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# load data using sklearn.datasets.load_files\n",
    "dataset = load_files(r\"C:\\Users\\veron\\OneDrive\\Documents\\LIN 373N\\HW3\\movie-reviews\")\n",
    "\n",
    "# split the data into train and test\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size = 0.2, random_state = 3)\n",
    "\n",
    "# vectorize the training data\n",
    "vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "X_train = vectorizer.fit_transform(docs_train)\n",
    "\n",
    "# fit the model with the training data\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# vectorize the test data and predict \n",
    "X_test = vectorizer.transform(docs_test)\n",
    "y_hat_nb = nb_model.predict(X_test)\n",
    "\n",
    "# get accuracy score\n",
    "accuracy_score_nb = metrics.accuracy_score(y_test, y_hat_nb)\n",
    "\n",
    "# print out some data\n",
    "print(\"nb_model accuracy is : \", accuracy_score_nb)\n",
    "print(\"classification report:\\n\", metrics.classification_report(y_test, y_hat_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(accuracy_score_nb == 0.8175)\n",
    "## If this throws and error, pleast contact the TA immediately!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some of the problems below, we have included the expected outputs (e.g., performance of the models) as `assertions` in the cells immediately below the cell where your implementation goes; they start with `## Do not edit this cell - autograder test`.\n",
    "\n",
    "**(a)** (10 points) Train a logistic regression model named `lr_model`, using the [logistic regression model provided by sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). How do `lr_model` and `nb_model` compare in terms of accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_model accuracy is :  0.8475\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84       197\n",
      "           1       0.83      0.88      0.85       203\n",
      "\n",
      "    accuracy                           0.85       400\n",
      "   macro avg       0.85      0.85      0.85       400\n",
      "weighted avg       0.85      0.85      0.85       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use random_state = 3, solver='liblinear'\n",
    "# From documentation\"For small datasets, âliblinearâ is a good choice, whereas âsagâ and âsagaâ are faster for large ones.\"\n",
    "# \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(random_state = 3, solver = 'liblinear')\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_hat_lr = lr_model.predict(X_test)\n",
    "accuracy_score_lr = metrics.accuracy_score(y_test, y_hat_lr)\n",
    "\n",
    "print(\"lr_model accuracy is : \", metrics.accuracy_score(y_test, y_hat_lr))\n",
    "print(\"classification report:\\n\", metrics.classification_report(y_test, y_hat_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visable test cases passed!\n"
     ]
    }
   ],
   "source": [
    "## Do not edit this cell - autograder test\n",
    "assert(lr_model.solver == 'liblinear')\n",
    "assert(lr_model.random_state == 3)\n",
    "assert(accuracy_score_lr == 0.8475)\n",
    "print(\"visable test cases passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which model is better?\n",
    "\n",
    "# YOUR CODE HERE: change \"None\" to be either \"nb\" or \"lr\"\n",
    "better_basic_model = \"lr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** (10 points) Instead of performing just one train/test split, try performing cross validation using the sklearn function `cross_val_score`. [See here for documentation on doing this](https://scikit-learn.org/stable/modules/cross_validation.html). Using 10-fold cross-validation, retrain and run both NB and LR. For cross validation, because the evaluator will automatically section your data for you, make sure to:\n",
    "* define and train new NB/LR models\n",
    "* fit your (new) vectorizer on the entire dataset\n",
    "* use the entire dataset in the socring function `cross_val_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Regression: 0.8470 (+/- 0.0383)\n",
      "Accuracy for Naive Bayes: 0.8095 (+/- 0.0635)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# use random_state = 3 and solver='liblinear'\n",
    "# For cross validation, make sure you are using the entire dataset as your \n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X, y = dataset.data, dataset.target\n",
    "vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "lr_xval_model = LogisticRegression(random_state = 3, solver = 'liblinear')\n",
    "# lr_xval_model.fit(X_train, y_train)\n",
    "# y_hat_lr_xval = lr_xval_model.predict(X_test)\n",
    "lr_xval_scores = cross_val_score(lr_xval_model, X, y, cv = 10)\n",
    "\n",
    "nb_xval_model = MultinomialNB()\n",
    "# nb_xval_model.fit(X_train, y_train)\n",
    "# y_hat_nb_xval = nb_xval_model.predict(X_test)\n",
    "nb_xval_scores = cross_val_score(nb_xval_model, X, y, cv = 10)\n",
    "\n",
    "\n",
    "print(\"Accuracy for Logistic Regression: %0.4f (+/- %0.4f)\" % (lr_xval_scores.mean(), lr_xval_scores.std() * 2))\n",
    "print(\"Accuracy for Naive Bayes: %0.4f (+/- %0.4f)\" % (nb_xval_scores.mean(), nb_xval_scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visable test cases passed!\n"
     ]
    }
   ],
   "source": [
    "## Do not edit this cell - autograder test\n",
    "assert(lr_xval_model.solver == 'liblinear')\n",
    "assert(lr_xval_model.random_state == 3)\n",
    "assert(round(lr_xval_scores.mean(), 4) == 0.8470)\n",
    "assert(round(nb_xval_scores.mean(), 4) == 0.8095)\n",
    "print(\"visable test cases passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which model is better?\n",
    "\n",
    "# YOUR CODE HERE: change \"None\" to be either \"nb\" or \"lr\"\n",
    "better_cv_model = \"lr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** (5 points) Now, let's modify the features in our classifier! The first thing we're trying is to have our classifier not only include unigrams, but also bigrams. Modify your `CountVectorizer` to include bigrams and use logistic regression as your classifier. Did the results on 10-fold cross validation improve? (Make sure to use the same folds as before, so the results are comparable!)\n",
    "\n",
    "*Hint*: take a look at the `CountVectorizer` documentation -- you only need to change one parameter in this function to enable bigrams! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Regression: 0.8500 (+/- 0.0443)\n",
      "Accuracy for Naive Bayes: 0.8085 (+/- 0.0707)\n"
     ]
    }
   ],
   "source": [
    "vectorizer_mod = CountVectorizer(stop_words = \"english\", ngram_range = (1, 2))\n",
    "X = vectorizer_mod.fit_transform(dataset.data)\n",
    "X_train = vectorizer_mod.fit_transform(docs_train)\n",
    "# X_test = vectorizer_mod.transform(docs_test)\n",
    "\n",
    "lr_bigram_model = LogisticRegression(random_state = 3, solver = 'liblinear')\n",
    "# lr_bigram_model.fit(X_train, y_train)\n",
    "# y_hat_lr_bigram = lr_bigram_model.predict(X_test)\n",
    "lr_bigram_scores = cross_val_score(lr_bigram_model, X, y, cv = 10)\n",
    "\n",
    "nb_bigram_model = MultinomialNB()\n",
    "# nb_bigram_model.fit(X_train, y_train)\n",
    "# y_hat_nb_bigram = nb_bigram_model.predict(X_test)\n",
    "nb_bigram_scores = cross_val_score(nb_bigram_model, X, y, cv = 10)\n",
    "\n",
    "\n",
    "print(\"Accuracy for Logistic Regression: %0.4f (+/- %0.4f)\" % (lr_bigram_scores.mean(), lr_bigram_scores.std() * 2))\n",
    "print(\"Accuracy for Naive Bayes: %0.4f (+/- %0.4f)\" % (nb_bigram_scores.mean(), nb_bigram_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visable test cases passed!\n"
     ]
    }
   ],
   "source": [
    "## Do not edit this cell - autograder test\n",
    "assert(lr_bigram_model.solver == 'liblinear')\n",
    "assert(lr_bigram_model.random_state == 3)\n",
    "assert(round(lr_bigram_scores.mean(), 4) == 0.8500)\n",
    "assert(round(nb_bigram_scores.mean(), 4) == 0.8085)\n",
    "print(\"visable test cases passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** (25 points) Let's also add some features. Specifically, we will add the count of part-of-speech (POS): for a particular document, how many nouns are there? How many verbs?... and so on.\n",
    "\n",
    "* **(d-i)** (7 points) write a function `get_pos_tags(doc)` that when given a document, returns the list of POS tags. Use NLTK for this.\n",
    "```\n",
    ">>> get_pos_tags(\"This is a test case with a two singular nouns.\")\n",
    "Counter({'DT': 3, 'NN': 2, 'VBZ': 1, 'IN': 1, 'CD': 1, 'JJ': 1, 'NNS': 1, '.': 1})\n",
    ">>> get_pos_tags(\"This is a test case with multiple sentences. Will you get it right?\")\n",
    "Counter({'DT': 2, 'NN': 2, '.': 2, 'PRP': 2, 'VBZ': 1, 'IN': 1, 'JJ': 1, 'NNS': 1, 'MD': 1, 'VB': 1, 'RB': 1})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'DT': 3, 'NN': 2, 'VBZ': 1, 'IN': 1, 'CD': 1, 'JJ': 1, 'NNS': 1, '.': 1})\n",
      "Counter({'DT': 2, 'NN': 2, '.': 2, 'PRP': 2, 'VBZ': 1, 'IN': 1, 'JJ': 1, 'NNS': 1, 'MD': 1, 'VB': 1, 'RB': 1})\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "def get_pos_tags(text) -> Counter:\n",
    "    \"\"\" when given a string, returns a POS tag counter, using NLTK\"\"\"\n",
    "    text = str(text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    pos_tags = []\n",
    "    for i in sentences:\n",
    "        tokens = nltk.word_tokenize(i)\n",
    "        pos_tags.extend(nltk.pos_tag(tokens))\n",
    "    counts = Counter(tag for token, tag in pos_tags)\n",
    "    return counts\n",
    "\n",
    "print(get_pos_tags(\"This is a test case with a two singular nouns.\"))\n",
    "print(get_pos_tags(\"This is a test case with multiple sentences. Will you get it right?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visable test cases passed!\n"
     ]
    }
   ],
   "source": [
    "## Do not edit this cell - autograder test\n",
    "assert(get_pos_tags(\"This is a test case with a two singular nouns.\")[\"NN\"] == 2)\n",
    "assert(get_pos_tags(\"This is a test case with multiple sentences. Will you get it right?\")[\"MD\"] == 1)\n",
    "assert(get_pos_tags(str(dataset.data[0]))[\"JJ\"] == 44)\n",
    "print(\"visable test cases passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_pos_features` in the cell below creates a DataFrame for the POS tags; you can visualize the outcome from the `print()` line at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    NN  ''  VBZ  VBN  DT  IN  NNS   ,  JJ  CD  ...  (  )  SYM  JJS  RP   EX  \\\n",
      "0   99   2   26   11  59  55   24  35  44   8  ...  5  5    2    2   2  1.0   \n",
      "1  155   2   23   11  63  72   35  26  55   4  ...  5  5    2    2   6  0.0   \n",
      "\n",
      "     :    $  RBS  PDT  \n",
      "0  0.0  0.0  0.0  0.0  \n",
      "1  5.0  1.0  2.0  1.0  \n",
      "\n",
      "[2 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "## Do not modify this cell, but do execute it.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_pos_features(docs: list) -> pd.DataFrame:\n",
    "    \n",
    "    _temp_df = pd.DataFrame(docs, columns=[\"docs\"])\n",
    "    _temp_df = _temp_df.docs.apply(get_pos_tags).to_list()\n",
    "    _df_pos = pd.DataFrame(_temp_df).fillna(0)\n",
    "    return _df_pos\n",
    "\n",
    "print(get_pos_features(dataset.data[:2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(d-ii)** (7 points) Now, write a function `transform_data` and use ColumnTransformer to combine *unigram* and POS features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "import scipy\n",
    "\n",
    "def transform_data(raw_data) -> pd.DataFrame:  \n",
    "        \n",
    "    pos = get_pos_features(raw_data)\n",
    "    \n",
    "    unigrams = pd.DataFrame({\"uni\": raw_data})\n",
    "    unigrams = unigrams.join(pos)\n",
    "\n",
    "    column_trans = ColumnTransformer([(\"words\", CountVectorizer(), \"uni\")], \n",
    "                                     remainder = 'passthrough')\n",
    "    \n",
    "    data = column_trans.fit_transform(unigrams)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visable test cases passed!\n"
     ]
    }
   ],
   "source": [
    "# Note, using ColumnTransformer will return a numpy array with a smaller amount of data, \n",
    "# but a scipi sparse matrix with more data.  \n",
    "\n",
    "## Do not edit this cell - autograder test\n",
    "assert(type(transform_data(dataset.data[:3])) == np.ndarray)\n",
    "assert(transform_data(dataset.data[:3])[0][5] == 1)\n",
    "assert(type(transform_data(dataset.data[:20])) == scipy.sparse.csr.csr_matrix)\n",
    "print(\"visable test cases passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(d-iii)** (6 points) Finally, re-train your logistic regression model over the combined features (run transform_data on the entire dataset first). What is the 10-fold cross validation accuracy? (Make sure to use the same folds as before, so the results are comparable! Feature extraction should finish in under 10 mins for the entire dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Regression: 0.8410 (+/- 0.0657)\n"
     ]
    }
   ],
   "source": [
    "#use random_state = 3\n",
    "\n",
    "X, y = dataset.data, dataset.target\n",
    "X = transform_data(X)\n",
    "\n",
    "lr_combined_model = LogisticRegression(random_state = 3, solver = 'liblinear')\n",
    "lr_combo_scores = cross_val_score(lr_combined_model, X, y, cv = 10)\n",
    "\n",
    "\n",
    "print(\"Accuracy for Logistic Regression: %0.4f (+/- %0.4f)\" % (lr_combo_scores.mean(), lr_combo_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(d-v)** (5 points) Some analysis: essentially, we have trained two models: LR with unigrams and bigrams, and LR with unigrams and POS tags. This setting allows us to compare the power of the new features (bigrams/POS tags). Which one is the better feature for the sentiment analysis task? Explain at least one linguistic reason why this may be true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for LR with unigrams and bigrams performed better than the LR with unigrams and POS tags. This model would make more sense for the sentiment analysis task since the sentiment of a sentence often depends on the word before, like saying \"pretty bad,\" or \"very awesome.\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
